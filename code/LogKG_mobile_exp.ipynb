{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from glove import Glove\n",
    "from glove import Corpus\n",
    "from collections import Counter\n",
    "from numpy import linalg as la\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cluster import OPTICS, AgglomerativeClustering, DBSCAN, KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import NearestCentroid, KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "from sklearn.semi_supervised import LabelPropagation,LabelSpreading\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "from model import LogCluster, LogKG, LogKGAblationKGE\n",
    "from numpy.lib.function_base import extract\n",
    "import os\n",
    "\n",
    "\n",
    "# Exp config\n",
    "\n",
    "CONFIG_PATH = os.path.join(\"../data\", \"config.json\")\n",
    "CASE_DIR = \"../data/CMCC_case\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the KGE of templates\n",
    "\n",
    "EMBEDDING_SIZE = 16\n",
    "template_embedding = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get case label\n",
    "def get_case_truth_label(case_name_list, input_config):\n",
    "    truth_label_list = np.zeros((len(case_name_list)), dtype=int)\n",
    "    label_config = {}\n",
    "    for index, fault_class in enumerate(input_config.keys()):\n",
    "        label_config[fault_class] = index\n",
    "        for case_name in input_config[fault_class]:\n",
    "            truth_label_list[case_name_list.index(case_name)] = index\n",
    "    return truth_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min samples in one cluster\n",
    "from sklearn import cluster\n",
    "\n",
    "\n",
    "min_samples = 3\n",
    "\n",
    "# cluster algorithm\n",
    "cluster_model = OPTICS(min_samples=min_samples, metric=\"cosine\", xi=0.05, algorithm=\"brute\")\n",
    "\n",
    "# FOLR threshold\n",
    "IDF_threshold = 0.4\n",
    "\n",
    "# classifier\n",
    "clf = RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate distance\n",
    "def compute_squared_EDM_method(X):\n",
    "  n,m = X.shape\n",
    "  D = np.zeros([n, n])\n",
    "  for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "      D[i,j] = la.norm(X[i, :] - X[j, :])\n",
    "      D[j,i] = D[i,j]\n",
    "  return D\n",
    "\n",
    "# Calculate centroid\n",
    "def get_centroid_index(cluster_embedding):\n",
    "    distance_array = np.sum(compute_squared_EDM_method(cluster_embedding), axis=1)\n",
    "    return np.argmin(distance_array)\n",
    "\n",
    "# Get LogKG result\n",
    "def get_logkg_result(train_set, train_index):\n",
    "    cluster_ = cluster_model.fit_predict(train_set)\n",
    "    class_num = np.max(cluster_) + 1\n",
    "    print(cluster_)\n",
    "    print(Counter(cluster_))\n",
    "    print(\"class_num: {}\".format(class_num))\n",
    "    return [train_index[np.where(cluster_==i)[0][get_centroid_index(train_set[np.where(cluster_==i)[0]])]] for i in range(class_num)], cluster_\n",
    "\n",
    "def LogKG_exp_run(case_name_list, case_truth_label,\n",
    "                  train_index, test_index, logkg_config):\n",
    "    logkg = logkg_config\n",
    "    logkg.get_train_embedding()\n",
    "    logkg.get_test_embedding()\n",
    "    train_embedding = logkg.train_embedding_dict\n",
    "    test_embedding = logkg.test_embedding_dict\n",
    "    train_set = np.array([train_embedding[case_name_list[index]] for index in train_index])\n",
    "    test_set = np.array([test_embedding[case_name_list[index]] for index in test_index])\n",
    "    \n",
    "    cluster_centroids, cluster_result = get_logkg_result(train_set=train_set, train_index=train_index)\n",
    "\n",
    "    classify_index = np.zeros(len(cluster_result)) - 1\n",
    "    for i in range(np.max(cluster_result) + 1):\n",
    "        class_label = case_truth_label[cluster_centroids[i]]\n",
    "        classify_index[np.where(cluster_result==i)[0]] = class_label\n",
    "        \n",
    "    # Classification\n",
    "    classify_train_label = classify_index[np.where(classify_index!=-1)[0]]\n",
    "    classify_train_set = train_set[np.where(classify_index!=-1)[0]]\n",
    "    classifier = clf()\n",
    "    \n",
    "    classifier.fit(classify_train_set, classify_train_label)\n",
    "    result = classifier.predict(test_set)\n",
    "    p_class, r_class, f_class, support_micro = precision_recall_fscore_support(case_truth_label[test_index], result.astype(np.int), average=None)\n",
    "    \n",
    "    print(\"accuracy_score:\", accuracy_score(case_truth_label[test_index], result.astype(np.int)), \"   f1_score:\", f1_score(case_truth_label[test_index], result.astype(np.int), average=\"macro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogKG:\n",
    "    def __init__(self, train_case_log_df:pd.DataFrame, test_case_log_df:pd.DataFrame, idf_threshold:float, template_embedding) -> None:\n",
    "        self.template_embedding = template_embedding\n",
    "        self.train_case_log_df = train_case_log_df\n",
    "        self.test_case_log_df = test_case_log_df\n",
    "        self.idf_threshold = idf_threshold\n",
    "        \n",
    "    def get_train_idf(self):\n",
    "        case_log_set_list = [list(set(df[\"EventId\"].values)) for df in self.train_case_log_df.values()]\n",
    "        case_all_template_occurrence = []\n",
    "        for case_log_set in case_log_set_list:\n",
    "            case_all_template_occurrence += case_log_set\n",
    "        case_log_template_counter = dict(Counter(case_all_template_occurrence))\n",
    "        self.template_list = list(case_log_template_counter.keys())\n",
    "        template_idf = {}\n",
    "        for template in case_log_template_counter:\n",
    "            idf = math.log10(len(case_log_set_list) / case_log_template_counter[template])\n",
    "            template_idf[template] = idf if idf > self.idf_threshold else 0.0\n",
    "        self.template_idf = template_idf\n",
    "        \n",
    "    def get_train_embedding(self):\n",
    "        self.get_train_idf()\n",
    "        case_embedding_dict = {}\n",
    "        for key in self.train_case_log_df:\n",
    "            embedding_array = np.zeros(len(self.template_idf), dtype=float)\n",
    "            log_df = self.train_case_log_df[key]\n",
    "            template_sequence = log_df[\"EventId\"].values\n",
    "            case_template_counter = dict(Counter(template_sequence))\n",
    "            important_log_count = 0\n",
    "            for template in case_template_counter:\n",
    "                if self.template_idf[template] != 0:\n",
    "                    important_log_count += case_template_counter[template]\n",
    "                else:\n",
    "                    case_template_counter[template] = 0\n",
    "            case_embedding = np.zeros(EMBEDDING_SIZE, dtype=np.float)\n",
    "            if important_log_count == 0:\n",
    "                case_embedding_dict[key] = case_embedding\n",
    "                continue\n",
    "            for template in case_template_counter:\n",
    "                case_embedding += (case_template_counter[template] / important_log_count) * self.template_idf[template] * self.template_embedding[template]\n",
    "            case_embedding_dict[key] = case_embedding\n",
    "        self.train_embedding_dict = case_embedding_dict\n",
    "\n",
    "    def get_test_embedding(self):\n",
    "        case_embedding_dict = {}\n",
    "        for key in self.test_case_log_df:\n",
    "            embedding_array = np.zeros(len(self.template_idf), dtype=float)\n",
    "            log_df = self.test_case_log_df[key]\n",
    "            template_sequence = log_df[\"EventId\"].values\n",
    "            case_template_counter = dict(Counter(template_sequence))\n",
    "            important_log_count = 0\n",
    "            for template in case_template_counter:\n",
    "                if template not in self.template_list:\n",
    "                    continue\n",
    "                if self.template_idf[template] != 0:\n",
    "                    important_log_count += case_template_counter[template]\n",
    "                else:\n",
    "                    case_template_counter[template] = 0\n",
    "            case_embedding = np.zeros(EMBEDDING_SIZE, dtype=np.float)\n",
    "            if important_log_count == 0:\n",
    "                case_embedding_dict[key] = case_embedding\n",
    "                continue\n",
    "            for template in case_template_counter:\n",
    "                if template not in self.template_list:\n",
    "                    continue\n",
    "                case_embedding += (case_template_counter[template] / important_log_count) * self.template_idf[template] * self.template_embedding[template]\n",
    "            case_embedding_dict[key] = case_embedding\n",
    "        self.test_embedding_dict = case_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_name_list = [name.split(\".\")[0] for name in os.listdir(CASE_DIR)]\n",
    "config = json.load(open(CONFIG_PATH))\n",
    "case_truth_label = get_case_truth_label(case_name_list, config)\n",
    "case_log_df = {case_name:pd.read_csv(os.path.join(CASE_DIR, case_name + \".csv\")) for case_name in case_name_list}\n",
    "\n",
    "# Set exp cases\n",
    "train_index = []\n",
    "test_index = []\n",
    "\n",
    "train_df = {case_name_list[index] : case_log_df[case_name_list[index]] for index in train_index} \n",
    "test_df = {case_name_list[index] : case_log_df[case_name_list[index]] for index in test_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-' * 30, \"LogKG\", '-' * 30)\n",
    "model = LogKG(train_df, test_df, IDF_threshold, template_embedding)\n",
    "LogKG_exp_run(case_name_list=case_name_list,\n",
    "              case_truth_label=case_truth_label,\n",
    "              train_index=train_index,\n",
    "              test_index=test_index,\n",
    "              logkg_config=model)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a2f9d27bf5c268f89df870aa9f8b2d154443c113e5aa893c583e723be5e85275"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('syc_tf2.2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
